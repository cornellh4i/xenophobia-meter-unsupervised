# -*- coding: utf-8 -*-
"""XMP-Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U72UD5lFvIu-y9P6u35EHCsuC5SgP-Tn
"""

from google.colab import drive
drive.mount('/content/drive')

# ALL MODULES
import pandas as pd
import nltk as nl 
import re
!pip install contractions
!pip install emoji --upgrade
from emoji import demojize
import contractions
from typing import List

"""# **Step 1 - Merge all csv files** 
- merge csv files using pandas
- drop all columns except column
- turn object into a pandas dataframe
"""

# merging all csv files
files =  ['/content/drive/MyDrive/XMP Preprocessing/AAAJ AAJC Cleaned - Final.csv', 
                      '/content/drive/MyDrive/XMP Preprocessing/AILANational Cleaned - Final.csv',
                      '/content/drive/MyDrive/XMP Preprocessing/BAJItweet Cleaned - Final.csv', 
                      '/content/drive/MyDrive/XMP Preprocessing/BreitbartNews Cleaned - Final.csv', 
                      '/content/drive/MyDrive/XMP Preprocessing/FAIRImmigration Cleaned - Final.csv', 
                      '/content/drive/MyDrive/XMP Preprocessing/ICEgov Cleaned - Final.csv', 
                      '/content/drive/MyDrive/XMP Preprocessing/IngrahamAngle Cleaned - Final.csv',
                      '/content/drive/MyDrive/XMP Preprocessing/StatePRM Cleaned - Final.csv',
                      '/content/drive/MyDrive/XMP Preprocessing/TuckerCarlson Cleaned - Final.csv',
                      '/content/drive/MyDrive/XMP Preprocessing/UNHCRUSA Cleaned - Final.csv', 
                      '/content/drive/MyDrive/XMP Preprocessing/splcenter Cleaned - Final.csv'] 

df = pd.concat(
    map(pd.read_csv, files), ignore_index=True)
df = df['Content']
df = df.to_frame()

print(df.head(20))

"""# **Step 2: Clean corpus with regular expressions**
- remove spaces
- remove urls
- remove numbers
- remove handles
- remove emojis
"""

def preprocess_tweet(tweet: str) -> str:
    """
    Performs Twitter-specific pre-processing steps.
    We do not remove punctuation right now because that would remove hashtags.
    """

    space_regex = r'\s+'
    url_regex = r"\w+:\/\/\S+"
    space_regex = r'\s+'
    number_regex = r'\d+'
    handle_regex = r"@[\w\d_]+"
    
    # replace long whitespace with single space
    cleaned_tweet = re.sub(space_regex, ' ', tweet)
    cleaned_tweet = re.sub(url_regex, '', cleaned_tweet)  # remove urls
    # remove user handles
    cleaned_tweet = re.sub(handle_regex, '', cleaned_tweet)
    cleaned_tweet = re.sub(number_regex, '', cleaned_tweet)  # remove numbers
    cleaned_tweet = demojize(cleaned_tweet)       # demojize

    cleaned_tweet = contractions.fix(cleaned_tweet).lower()
    return cleaned_tweet

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
for index, row in df.iterrows():
  df.at[index,'Content']=preprocess_tweet(row["Content"])

print(df.head(30))

"""# **Step 3: Tokenize Tweets**
- tokenize tweets using tweet tokenizer
- lemmatize 

"""

def tokenize(tweet: str, method: str = "tweet") -> List[str]:
    """
    Returns tokens of a tweet, tokenized using a specified method, with all the tokens stemmed.
    tweet is a string
    method is a string
    """

    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import TweetTokenizer, word_tokenize
    from nltk.stem import PorterStemmer
    import emoji
    import string
      
    lemmatizer = WordNetLemmatizer()

    tokens = []
    if method == "tweet":
        tweet_tokenizer = TweetTokenizer()
        tokens = tweet_tokenizer.tokenize(tweet)
    elif method == "word":
        tokens = word_tokenize(tweet)

    # remove tokens that are just punctuation

    # ***TODO: append other excluded ASCII values to string.punctuation***
    punctuation_list = [punct for punct in string.punctuation]
    punctuation_list.append(['’', '“'])
    tokens_punctuation_removed = [
        t for t in tokens if t not in punctuation_list]
        
    # remove emojis
    # tokens_emojis_removed = [
    #     t for t in tokens_punctuation_removed if t not in emoji.UNICODE_EMOJI]

    # uncomment to perform stemming
    # return [PorterStemmer().stem(t) for t in tokens_punctuation_removed]

    return [lemmatizer.lemmatize(w) for w in tokens_punctuation_removed]

for index, row in df.iterrows():
  df.at[index,'Content']=tokenize(row["Content"])

print(df.head(20))

"""# **Step 4: Remove apostrophes with Regular expressions**
- note: initially handles were of the form '@akin's', so removing handles
left us with "'s", now remove apostrophe leaves us with just "S", is this import
ant enough to affect our model?
"""

def remove_apostrophe(tweet):
  special_character_regex = r'[^\w ]'
  cleaned = ''
  result = []
  if tweet:
    for word in tweet:
      cleaned =  re.sub(special_character_regex, '', word)
      if cleaned == 's':
        cleaned = ''
      if len(cleaned) != 0:
        result.append(cleaned)
    return result

for index, row in df.iterrows():
  df.at[index,'Content']=remove_apostrophe(row["Content"])

print(df.head(10))

# Remove Null values
df.fillna("", inplace = True)

"""## **Step 5: Vectorization**"""

from gensim.models import Word2Vec

df_to_list = df['Content'].values.tolist()
model = Word2Vec(df_to_list, min_count=1)
words = list(model.wv.vocab)
model.save('model.bin')

print(words[:5])

# Vector for one word. 
print(model['southeast'])

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

data = model[model.wv.vocab]

pca = PCA(n_components=2)
result = pca.fit_transform(data)

# Create Scatter plot to visualize words
plt.scatter(result[:20, 0], result[:20, 1])
words = list(model.wv.vocab)
for i, word in enumerate(words[:20]):
  plt.annotate(word, xy=(result[i, 0], result[i,1]))
plt.show()

# Pickle dataframe
df.to_pickle('/content/drive/MyDrive/XMP Preprocessing/cleaned_data.pkl')